{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project - 4 : Treasure Hunters Inc.\n",
    "\n",
    "# A game where the player in a map has to reach the treasure overcoming the monsters and blockers in between the path.\n",
    "# Goal : Get the optimal path from source to destination.\n",
    "\n",
    "This implementation differs from previous implementation in 2 ways..\n",
    "1. Using Epsilon - Greedy method to generate the policy \n",
    "2. The rewards are different in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps in Project\n",
    "\n",
    "### 1. Initialize transition,action and rewards tables\n",
    "### 2. Build and Run the Reinforcement learning model on the map.\n",
    "### 3. Build the QL table\n",
    "### 4. Identify the optimal path from QLearning table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the 25 x 5 transitions table with possible moves.\n",
    "\n",
    "### There are 25 possible positions on the game board, \n",
    "### we are going to define a possible position to move for each of the positions in each direction.\n",
    "### For each position we have 4 possible moves, UP, DOWN, LEFT, RIGHT, so each position in game board, player will have 4 possible ways to move.\n",
    "### every invalid move has a -1.\n",
    "### we account for invalid move when the move is not possible, \n",
    "### for example if we are on first row, going up is an invalid move.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def intializeTransitions():\n",
    "    \n",
    "    transitions = np.zeros((25,5))\n",
    "    for i in range(25):    \n",
    "        for j in range(4):\n",
    "            \n",
    "            # evaluation for moving UP\n",
    "            # we always have 0-4 values in first row, so if i < 5 then we are on the top row, so going up\n",
    "            # from top row is invalid.\n",
    "            # from any other value, moving up is just subtracting 5 from current position.\n",
    "            if i < 5:\n",
    "                transitions[i][0] = -1\n",
    "            else:\n",
    "                transitions[i][0] = i - 5\n",
    "                \n",
    "            # evaluation for moving DOWN\n",
    "            # we always have 20-24 values in last row, so if i > 20 then we are on the bottom most row, so going DOWN\n",
    "            # from bottom row is invalid.\n",
    "            # from any other value, moving DOwN is just adding 5 from current position.\n",
    "            \n",
    "            if i >= 20:\n",
    "                transitions[i][1] = -1\n",
    "            else:\n",
    "                transitions[i][1] = i + 5\n",
    "                \n",
    "            # evaluation for moving LEFT\n",
    "            # we always have values divisible by 5 in first column, so if i % 5 ==0 then we are on the first column, \n",
    "            # so going LEFT is not possible.\n",
    "            # from any other value, moving LEFT is just adding -1 from current position.\n",
    "            \n",
    "            if i % 5 == 0:\n",
    "                transitions[i][2] = -1\n",
    "            else:\n",
    "                transitions[i][2] = i - 1\n",
    "            \n",
    "            # evaluation for moving RIGHT\n",
    "            # we always have values divisible by 5 in the (last column + 1), so if (i+1) % 5 ==0 then we are on the last column, \n",
    "            # so going RIGHT is not possible.\n",
    "            # from any other value, moving RIGHT is just adding +1 from current position.\n",
    "            \n",
    "            # check if rightmost column, then 'right' action invalid\n",
    "            if (i+1) % 5 == 0:\n",
    "                transitions[i][3] = -1\n",
    "            else:\n",
    "                transitions[i][3] = i + 1\n",
    "            \n",
    "            # Lets store the current positions as well.\n",
    "            transitions[i][4] = i\n",
    "    \n",
    "    transitions = transitions.astype(int)\n",
    "    return transitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewards Policy\n",
    "\n",
    "### Initializing the 25 x 5 rewards table with possible transitions from transitions table and the game board.\n",
    "\n",
    "### The Policy : \n",
    "### There are 25 possible position on the game board, \n",
    "### we are going to define a rewards for each possible state in the grid.\n",
    "### Defining the rewards policy..\n",
    "### Whenever the player encounters a Monster, the reward is -20\n",
    "### Whenever the player encounters a Blocker, the reward is -20\n",
    "### Whenever the player encounters the Treasure, the reward is +100\n",
    "### Whenever the Player encounters a * , the reward is +5 so that the Player can move. \n",
    "### Invalid moves from transition tables get a 0 reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeRewards(gameBoard, transitions):\n",
    "    \n",
    "    rewards = np.zeros((25,5))\n",
    "    \n",
    "    for i in range(25):\n",
    "        \n",
    "        for j in range(5):\n",
    "            \n",
    "            possibleState = transitions[i][j]\n",
    "            \n",
    "            # if possible state is an invalid move = -1 from tansition table,\n",
    "            # then set the reward for that state as 0.\n",
    "            if possibleState == -1:\n",
    "                rewards[i][j] = 0\n",
    "                continue\n",
    "            \n",
    "            # if its a neutral move, then reward is 0\n",
    "            if possibleState == i:\n",
    "                rewards[i][j] = 0\n",
    "                continue\n",
    "            \n",
    "            # if possible state encounter a Monster or a Blocker, set the reward to -50 or -30 respectively.\n",
    "            gameBoardRow = int(possibleState / 5)\n",
    "            gameBoardColumn = int(possibleState % 5)\n",
    "            if gameBoard[gameBoardRow][gameBoardColumn] == 'M':\n",
    "                rewards[i][j] = -10\n",
    "            elif gameBoard[gameBoardRow][gameBoardColumn] == 'B':\n",
    "                rewards[i][j] = -5\n",
    "            elif gameBoard[gameBoardRow][gameBoardColumn] == 'T':\n",
    "                rewards[i][j] = 10\n",
    "            elif gameBoard[gameBoardRow][gameBoardColumn] == '*':\n",
    "                rewards[i][j] = 1\n",
    "\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initializing an array of possible actions for all possible positions on the given game board.\n",
    "#### Hardcoding these values as they are constant, we can programmatically write this as well.\n",
    "0 - UP\n",
    "1 - DOWN\n",
    "2 - LEFT\n",
    "3 - RIGHT\n",
    "4 - NO MOVES\n",
    "For example : the first value has 1,3,4 indicating that the player at that position can move DOWN or RIGHT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializePossibleActions():\n",
    "    possibleActions = np.array([[1, 3, 4],\n",
    "                               [1, 2, 3, 4],\n",
    "                               [1, 2, 3, 4],\n",
    "                               [1, 2, 3, 4],\n",
    "                               [1, 2, 4],\n",
    "                               [0, 1, 3, 4],\n",
    "                               [0, 1, 2, 3, 4],\n",
    "                               [0, 1, 2, 3, 4],\n",
    "                               [0, 1, 2, 3, 4],\n",
    "                               [0, 1, 2, 4],\n",
    "                               [0, 1, 3, 4],\n",
    "                               [0, 1, 2, 3, 4],\n",
    "                               [0, 1, 2, 3, 4],\n",
    "                               [0, 1, 2, 3, 4],\n",
    "                               [0, 1, 2, 4],\n",
    "                               [0, 1, 3, 4],\n",
    "                               [0, 1, 2, 3, 4],\n",
    "                               [0, 1, 2, 3, 4],\n",
    "                               [0, 1, 2, 3, 4],\n",
    "                               [0, 1, 2, 4],\n",
    "                               [0, 3, 4],\n",
    "                               [0, 2, 3, 4],\n",
    "                               [0, 2, 3, 4],\n",
    "                               [0, 2, 3, 4],\n",
    "                                [0, 2, 4]])\n",
    "    return possibleActions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the game board with all required data.\n",
    "\n",
    "#### Note that our game board is 5 X 5, so on total we will have 25 possibile positions on the board.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setupGame(game):\n",
    "    transitions = intializeTransitions();\n",
    "    rewards = initializeRewards(game, transitions)\n",
    "    return rewards, transitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforced Learning model - Epsilon Greedy Method\n",
    "\n",
    "Exploration vs Exploitation\n",
    "\n",
    "Exploration allows an agent to improve its current knowledge about each action, hopefully leading to long-term benefit. Improving the accuracy of the estimated action-values, enables an agent to make more informed decisions in the future.\n",
    "\n",
    "Exploitation on the other hand, chooses the greedy action to get the most reward by exploiting the agent’s current action-value estimates. But by being greedy with respect to action-value estimates, may not actually get the most reward and lead to sub-optimal behaviour.\n",
    "When an agent explores, it gets more accurate estimates of action-values. And when it exploits, it might get more reward. It cannot, however, choose to do both simultaneously, which is also called the exploration-exploitation dilemma.\n",
    "\n",
    "Epsilon-Greedy Action Selection\n",
    "Epsilon-Greedy is a simple method to balance exploration and exploitation by choosing between exploration and exploitation randomly.\n",
    "The epsilon-greedy, where epsilon refers to the probability of choosing to explore, exploits most of the time with a small chance of exploring.\n",
    "\n",
    "** src : https://www.geeksforgeeks.org/epsilon-greedy-algorithm-in-reinforcement-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def reinforcedLearningEpsilonGreedyModel(gameBoard, transitions, rewards, learningRate, discountFactor, epochs):\n",
    "    actionsAvailable = initializePossibleActions()\n",
    "    qLearnings = np.zeros((25,5))\n",
    "\n",
    "    for i in range(epochs):\n",
    "    \n",
    "        initialState = 0\n",
    "        terminalState = 24\n",
    "        currentState = initialState\n",
    "        #Keep moving forward until the goal state is reached\n",
    "        while currentState != terminalState:\n",
    "            # random choice of action at every particular state.\n",
    "            action = random.choice(actionsAvailable[currentState])\n",
    "\n",
    "            #move to the next state based on  randomly chosen action and transitions.\n",
    "            nextState = transitions[currentState][action]\n",
    "            futureRewards = []\n",
    "\n",
    "            #identify and Add all rewards for all future actions..\n",
    "            for nextPossibleAction in actionsAvailable[nextState]:\n",
    "                futureRewards.append(qLearnings[nextState][nextPossibleAction])\n",
    "\n",
    "            ## Identify maximum Q learning value and apply the formula\n",
    "            qPrevious = qLearnings[currentState][action] \n",
    "            qValueToUpdate = (1 - learningRate) *  qPrevious + learningRate * (rewards[currentState][action]  + discountFactor * max(futureRewards))\n",
    "            \n",
    "            ## Exploration vs Exploitation.. GREEDY EPSILON MODEL\n",
    "            probability = np.random.random()\n",
    "            if probability < 0.1: #(0.1 is epsilon here..)\n",
    "                qValueToUpdate = np.random.choice(3) # exploitation use case..\n",
    "            else:\n",
    "                # exploration usecase..\n",
    "                qValueToUpdate = (1 - learningRate) *  qPrevious + learningRate * (rewards[currentState][action]  + discountFactor * max(futureRewards))\n",
    "            \n",
    "            #Update the Q table with new reward value\n",
    "            qLearnings[currentState][action] = qValueToUpdate\n",
    "\n",
    "            # go to next state.\n",
    "            currentState = nextState\n",
    "    return qLearnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initital design for the map to Treasure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P - Player.\n",
    "# M - Monster\n",
    "# B - Blocker\n",
    "# T - Treasure\n",
    "# Help the Player to reach treasure.\n",
    "\n",
    "mapToTreasure = [['P','*','*','M','*'],\n",
    "                 ['*','B','M','*','*'],\n",
    "                 ['*','*','*','*','M'],\n",
    "                 ['B','*','B','*','M'],\n",
    "                 ['*','*','*','*','T']]\n",
    "\n",
    "rewards, transitions = setupGame(mapToTreasure)\n",
    "play = reinforcedLearningEpsilonGreedyModel(gameBoard=mapToTreasure,transitions=transitions, rewards=rewards, learningRate = 0.5, discountFactor = 0.8, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Play move scores..\n",
      "           0          1         2          3         4\n",
      "0   0.000000   5.509124  0.000000   4.420045  4.450867\n",
      "1   0.000000  -0.707360  4.410433   3.849323  3.214497\n",
      "2   0.000000  -3.447153  4.306623  -2.948441  2.774445\n",
      "3   0.000000   6.754018  3.638970   5.394073  0.000000\n",
      "4   0.000000   6.058478 -3.773465   0.000000  4.573018\n",
      "5   0.000000   5.341170  0.000000  -1.331227  4.234074\n",
      "6   4.371993   6.070089  1.000000  -4.391573  2.000000\n",
      "7   3.809735   6.613494 -0.680430   6.756511  5.397445\n",
      "8   2.000000   7.197846 -4.601649   6.031058  0.000000\n",
      "9   3.923391  -4.880137  6.547081   0.000000  5.098762\n",
      "10  3.136468   0.084173  0.000000   5.300657  4.371718\n",
      "11 -0.553384   6.880486  5.280176   6.094064  5.422702\n",
      "12 -4.430608   1.419706  5.653162   6.453120  0.000000\n",
      "13  6.731366   2.000000  6.388735  -4.911876  5.783783\n",
      "14  6.095322  -2.000000  6.402867   0.000000  4.979724\n",
      "15  5.163299   5.974866  0.000000   6.649916  5.195237\n",
      "16  6.346048   7.447651  0.092730   1.399764  5.163542\n",
      "17  5.992637   8.046872  6.533724   8.046875  6.385349\n",
      "18  6.565965   8.992187  1.295540  -1.996094  7.058845\n",
      "19 -3.716858  10.000000  7.162500   0.000000  5.000000\n",
      "20  0.112493   0.000000  0.000000   3.755343  5.456983\n",
      "21  6.772969   0.000000  5.593364   8.158242  6.497589\n",
      "22  1.234163   0.000000  4.263297   8.972656  0.000000\n",
      "23  4.093750   0.000000  7.948046  10.000000  7.999634\n",
      "24  0.000000   0.000000  0.000000   0.000000  0.000000\n",
      "Transitions..\n",
      "[[-1  5 -1  1  0]\n",
      " [-1  6  0  2  1]\n",
      " [-1  7  1  3  2]\n",
      " [-1  8  2  4  3]\n",
      " [-1  9  3 -1  4]\n",
      " [ 0 10 -1  6  5]\n",
      " [ 1 11  5  7  6]\n",
      " [ 2 12  6  8  7]\n",
      " [ 3 13  7  9  8]\n",
      " [ 4 14  8 -1  9]\n",
      " [ 5 15 -1 11 10]\n",
      " [ 6 16 10 12 11]\n",
      " [ 7 17 11 13 12]\n",
      " [ 8 18 12 14 13]\n",
      " [ 9 19 13 -1 14]\n",
      " [10 20 -1 16 15]\n",
      " [11 21 15 17 16]\n",
      " [12 22 16 18 17]\n",
      " [13 23 17 19 18]\n",
      " [14 24 18 -1 19]\n",
      " [15 -1 -1 21 20]\n",
      " [16 -1 20 22 21]\n",
      " [17 -1 21 23 22]\n",
      " [18 -1 22 24 23]\n",
      " [19 -1 23 -1 24]]\n"
     ]
    }
   ],
   "source": [
    "# converting the data into dataframe for manipulation purposes.\n",
    "play = pd.DataFrame(play)\n",
    "\n",
    "## Lets print the play move scores..\n",
    "print(\"Play move scores..\")\n",
    "print(play)\n",
    "\n",
    "print(\"Transitions..\")\n",
    "print(transitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the route from the run.\n",
    "\n",
    "### if at all there is going to be any visited node already then, pick the next highly possible move from all the moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 --> 5 --> 10 --> 11 --> 16 --> 21 --> 22 --> 23 --> "
     ]
    }
   ],
   "source": [
    "optimalrouteToTreasure = []\n",
    "currentstate = 0\n",
    "previousState=0\n",
    "nextHighestValue=0\n",
    "state = 0\n",
    "\n",
    "while state != 24: #(terminal state)\n",
    "    row = play.iloc[state]\n",
    "    print(state, \"-->\", end=\" \")\n",
    "    # get the index maximum value from the dataframe row\n",
    "    action = row.idxmax(axis=1)\n",
    "    #transitions of current state and the action will identify the next state\n",
    "    state = transitions[state][action]\n",
    "\n",
    "    ## trying to avoid previously visited nodes if any.\n",
    "    if (state in optimalrouteToTreasure):\n",
    "        nextHighest = 4 #(next highest in 5(0-4) values is 3, so setting this to 4 and subtracting below)\n",
    "        while(state in optimalrouteToTreasure):\n",
    "            nextHighest=nextHighest-1\n",
    "            nextHighestValue = np.sort(play.iloc[previousState])[nextHighest] # get next highest value\n",
    "            i=0\n",
    "            for value in row.tolist():    \n",
    "                if value == nextHighestValue:\n",
    "                    state = transitions[previousState][i]\n",
    "                    break\n",
    "                i=i+1\n",
    "                \n",
    "    optimalrouteToTreasure.append(state)\n",
    "    previousState = state    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal route for the Player to reach the treasure from source  is  [5, 10, 11, 16, 21, 22, 23, 24]\n"
     ]
    }
   ],
   "source": [
    "print(\"The optimal route for the Player to reach the treasure from source  is \", optimalrouteToTreasure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
