{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the 25 x 5 transitions table with possible moves.\n",
    "\n",
    "### There are 25 possible positions on the game board, \n",
    "### we are going to define a possible position to move for each of the positions in each direction.\n",
    "### For each position we have 4 possible moves, UP, DOWN, LEFT, RIGHT, so each position in game board, player will have 4 possible ways to move.\n",
    "### every invalid move has a -1.\n",
    "### we account for invalid move when the move is not possible, \n",
    "### for example if we are on first row, going up is an invalid move.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def intializeTransitions():\n",
    "    \n",
    "    transitions = np.zeros((25,5))\n",
    "    for i in range(25):    \n",
    "        for j in range(4):\n",
    "            \n",
    "            # evaluation for moving UP\n",
    "            # we always have 0-4 values in first row, so if i < 5 then we are on the top row, so going up\n",
    "            # from top row is invalid.\n",
    "            # from any other value, moving up is just subtracting 5 from current position.\n",
    "            if i < 5:\n",
    "                transitions[i][0] = -1\n",
    "            else:\n",
    "                transitions[i][0] = i - 5\n",
    "                \n",
    "            # evaluation for moving DOWN\n",
    "            # we always have 20-24 values in last row, so if i > 20 then we are on the bottom most row, so going DOWN\n",
    "            # from bottom row is invalid.\n",
    "            # from any other value, moving DOwN is just adding 5 from current position.\n",
    "            \n",
    "            if i >= 20:\n",
    "                transitions[i][1] = -1\n",
    "            else:\n",
    "                transitions[i][1] = i + 5\n",
    "                \n",
    "            # evaluation for moving LEFT\n",
    "            # we always have values divisible by 5 in first column, so if i % 5 ==0 then we are on the first column, \n",
    "            # so going LEFT is not possible.\n",
    "            # from any other value, moving LEFT is just adding -1 from current position.\n",
    "            \n",
    "            if i % 5 == 0:\n",
    "                transitions[i][2] = -1\n",
    "            else:\n",
    "                transitions[i][2] = i - 1\n",
    "            \n",
    "            # evaluation for moving RIGHT\n",
    "            # we always have values divisible by 5 in the (last column + 1), so if (i+1) % 5 ==0 then we are on the last column, \n",
    "            # so going RIGHT is not possible.\n",
    "            # from any other value, moving RIGHT is just adding +1 from current position.\n",
    "            \n",
    "            # check if rightmost column, then 'right' action invalid\n",
    "            if (i+1) % 5 == 0:\n",
    "                transitions[i][3] = -1\n",
    "            else:\n",
    "                transitions[i][3] = i + 1\n",
    "            \n",
    "            # Lets store the current positions as well.\n",
    "            transitions[i][4] = i\n",
    "    \n",
    "    transitions = transitions.astype(int)\n",
    "    return transitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewards Policy\n",
    "\n",
    "### Initializing the 25 x 5 rewards table with possible transitions from transitions table and the game board.\n",
    "\n",
    "### The Policy : \n",
    "### There are 25 possible position on the game board, \n",
    "### we are going to define a rewards for each possible state in the grid.\n",
    "### Defining the rewards policy..\n",
    "### Whenever the player encounters a Monster, the reward is -50\n",
    "### Whenever the player encounters a Blocker, the reward is -20\n",
    "### Whenever the player encounters the Treasure, the reward is +100\n",
    "### Whenever the Player encounters a * , the reward is +10 so that the Player can move. \n",
    "### Invalid moves from transition tables get a 0 reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeRewards(gameBoard, transitions):\n",
    "    \n",
    "    rewards = np.zeros((25,5))\n",
    "    \n",
    "    for i in range(25):\n",
    "        \n",
    "        for j in range(5):\n",
    "            \n",
    "            possibleState = transitions[i][j]\n",
    "            \n",
    "            # if possible state is an invalid move = -1 from tansition table,\n",
    "            # then set the reward for that state as 0.\n",
    "            if possibleState == -1:\n",
    "                rewards[i][j] = 0\n",
    "                continue\n",
    "            \n",
    "            # if its a neutral move, then reward is 0\n",
    "            if possibleState == i:\n",
    "                rewards[i][j] = 0\n",
    "                continue\n",
    "            \n",
    "            # if possible state encounter a Monster or a Blocker, set the reward to -50 or -30 respectively.\n",
    "            gameBoardRow = int(possibleState / 5)\n",
    "            gameBoardColumn = int(possibleState % 5)\n",
    "            if gameBoard[gameBoardRow][gameBoardColumn] == 'M':\n",
    "                rewards[i][j] = -10\n",
    "            elif gameBoard[gameBoardRow][gameBoardColumn] == 'B':\n",
    "                rewards[i][j] = -5\n",
    "            elif gameBoard[gameBoardRow][gameBoardColumn] == 'T':\n",
    "                rewards[i][j] = 10\n",
    "            elif gameBoard[gameBoardRow][gameBoardColumn] == '*':\n",
    "                rewards[i][j] = 1\n",
    "\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initializing an array of possible actions for all possible positions on the given game board.\n",
    "#### Hardcoding these values as they are constant, we can programmatically write this as well.\n",
    "0 - UP\n",
    "1 - DOWN\n",
    "2 - LEFT\n",
    "3 - RIGHT\n",
    "4 - NO MOVES\n",
    "For example : the first value has 1,3,4 indicating that the player at that position can move DOWN or RIGHT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializePossibleActions():\n",
    "    possibleActions = np.array([[1, 3, 4],\n",
    "                               [1, 2, 3, 4],\n",
    "                               [1, 2, 3, 4],\n",
    "                               [1, 2, 3, 4],\n",
    "                               [1, 2, 4],\n",
    "                               [0, 1, 3, 4],\n",
    "                               [0, 1, 2, 3, 4],\n",
    "                               [0, 1, 2, 3, 4],\n",
    "                               [0, 1, 2, 3, 4],\n",
    "                               [0, 1, 2, 4],\n",
    "                               [0, 1, 3, 4],\n",
    "                               [0, 1, 2, 3, 4],\n",
    "                               [0, 1, 2, 3, 4],\n",
    "                               [0, 1, 2, 3, 4],\n",
    "                               [0, 1, 2, 4],\n",
    "                               [0, 1, 3, 4],\n",
    "                               [0, 1, 2, 3, 4],\n",
    "                               [0, 1, 2, 3, 4],\n",
    "                               [0, 1, 2, 3, 4],\n",
    "                               [0, 1, 2, 4],\n",
    "                               [0, 3, 4],\n",
    "                               [0, 2, 3, 4],\n",
    "                               [0, 2, 3, 4],\n",
    "                               [0, 2, 3, 4],\n",
    "                                [0, 2, 4]])\n",
    "    return possibleActions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the game board with all required data.\n",
    "\n",
    "#### Note that our game board is 5 X 5, so on total we will have 25 possibile positions on the board.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setupGame(game):\n",
    "    transitions = intializeTransitions();\n",
    "    rewards = initializeRewards(game, transitions)\n",
    "    return rewards, transitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforced Learning model..\n",
    "\n",
    "Alpha is the learning rate (fine tuned from multiple runs..)\n",
    "\n",
    "Gamma is the discount factor. It quantifies how much importance we give for future rewards. \n",
    "Itâ€™s also handy to approximate the noise in future rewards. Gamma varies from 0 to 1. \n",
    "If Gamma is closer to zero, the agent will tend to consider only immediate rewards. \n",
    "If Gamma is closer to one, the agent will consider future rewards with greater weight,\n",
    "willing to delay the reward.\n",
    "In my implementation I'm keeping it close to 1 (0.8) to consider furture rewards as well.\n",
    "\n",
    "** src : https://towardsdatascience.com/practical-reinforcement-learning-02-getting-started-with-q-learning-582f63e4acd9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def reinforcedLearningModel(gameBoard, transitions, rewards, learningRate, discountFactor, epochs):\n",
    "    actionsAvailable = initializePossibleActions()\n",
    "    qLearnings = np.zeros((25,5))\n",
    "\n",
    "    for i in range(epochs):\n",
    "    \n",
    "        initialState = 0\n",
    "        terminalState = 4\n",
    "        currentState = initialState\n",
    "        #Keep moving forward until the goal state is reached\n",
    "        while currentState != terminalState:\n",
    "            # random choice of action at every particular state.\n",
    "            action = random.choice(actionsAvailable[currentState])\n",
    "\n",
    "            #move to the next state based on  randomly chosen action and transitions.\n",
    "            nextState = transitions[currentState][action]\n",
    "            futureRewards = []\n",
    "\n",
    "            #identify and Add all rewards for all future actions..\n",
    "            for nextPossibleAction in actionsAvailable[nextState]:\n",
    "                futureRewards.append(qLearnings[nextState][nextPossibleAction])\n",
    "\n",
    "            ## Identify maximum Q learning value and apply the formula\n",
    "            qPrevious = qLearnings[currentState][action] \n",
    "            qValueToUpdate = (1 - learningRate) *  qPrevious + learningRate * (rewards[currentState][action]  + discountFactor * max(futureRewards))\n",
    "\n",
    "            #Update the Q table with new reward value\n",
    "            qLearnings[currentState][action] = qValueToUpdate\n",
    "\n",
    "            # go to next state.\n",
    "            currentState = nextState\n",
    "    return qLearnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initital design for the map to Treasure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P - Player.\n",
    "# M - Monster\n",
    "# B - Blocker\n",
    "# T - Treasure\n",
    "# Help the Player to reach treasure.\n",
    "\n",
    "mapToTreasure = [['P','*','*','M','T'],\n",
    "                 ['*','B','M','*','*'],\n",
    "                 ['*','*','*','*','M'],\n",
    "                 ['B','*','B','*','M'],\n",
    "                 ['*','*','*','*','*']]\n",
    "\n",
    "rewards, transitions = setupGame(mapToTreasure)\n",
    "play = reinforcedLearningModel(gameBoard=mapToTreasure,transitions=transitions, rewards=rewards, learningRate = 0.5, discountFactor = 0.8, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Play move scores..\n",
      "            0         1         2          3         4\n",
      "0    0.000000  6.048576  0.000000   5.000000  4.838861\n",
      "1    0.000000  0.310720  4.838861   5.000000  4.000000\n",
      "2    0.000000 -3.440000  5.000000  -2.000000  4.000000\n",
      "3    0.000000  8.200000  5.000000  10.000000  8.000000\n",
      "4    0.000000  0.000000  0.000000   0.000000  0.000000\n",
      "5    4.838861  6.310720  0.000000   0.310720  5.048576\n",
      "6    5.000000  6.638400  6.048576  -3.440000  5.310720\n",
      "7    5.000000  7.048000  0.310720   8.200000  6.560000\n",
      "8   -2.000000  7.560000 -3.440000   9.000000  7.200000\n",
      "9   10.000000 -2.800000  8.200000   0.000000  8.000000\n",
      "10   6.048576  0.048576  0.000000   6.638400  5.310720\n",
      "11   0.310720  6.310720  6.310720   7.048000  5.638400\n",
      "12  -3.440000  0.638400  6.638400   7.560000  6.048000\n",
      "13   8.200000  7.048000  7.048000  -2.800000  6.560000\n",
      "14   9.000000 -4.361600  7.560000   0.000000  7.200000\n",
      "15   6.310720  5.838861  0.000000   6.310720  5.048576\n",
      "16   6.638400  6.048576  0.048576   0.638400  5.310720\n",
      "17   7.048000  6.310720  6.310720   7.048000  5.638400\n",
      "18   7.560000  6.638400  0.638400  -4.361600  6.048000\n",
      "19  -2.800000  6.310720  7.048000   0.000000  5.638400\n",
      "20   0.048576  0.000000  0.000000   6.048576  4.838861\n",
      "21   6.310720  0.000000  5.838861   6.310720  5.048576\n",
      "22   0.638400  0.000000  6.048576   6.638400  5.310720\n",
      "23   7.048000  0.000000  6.310720   6.310720  5.638400\n",
      "24  -4.361600  0.000000  6.638400   0.000000  5.310720\n",
      "Transitions..\n",
      "[[-1  5 -1  1  0]\n",
      " [-1  6  0  2  1]\n",
      " [-1  7  1  3  2]\n",
      " [-1  8  2  4  3]\n",
      " [-1  9  3 -1  4]\n",
      " [ 0 10 -1  6  5]\n",
      " [ 1 11  5  7  6]\n",
      " [ 2 12  6  8  7]\n",
      " [ 3 13  7  9  8]\n",
      " [ 4 14  8 -1  9]\n",
      " [ 5 15 -1 11 10]\n",
      " [ 6 16 10 12 11]\n",
      " [ 7 17 11 13 12]\n",
      " [ 8 18 12 14 13]\n",
      " [ 9 19 13 -1 14]\n",
      " [10 20 -1 16 15]\n",
      " [11 21 15 17 16]\n",
      " [12 22 16 18 17]\n",
      " [13 23 17 19 18]\n",
      " [14 24 18 -1 19]\n",
      " [15 -1 -1 21 20]\n",
      " [16 -1 20 22 21]\n",
      " [17 -1 21 23 22]\n",
      " [18 -1 22 24 23]\n",
      " [19 -1 23 -1 24]]\n"
     ]
    }
   ],
   "source": [
    "# converting the data into dataframe for manipulation purposes.\n",
    "play = pd.DataFrame(play)\n",
    "\n",
    "## Lets print the play move scores..\n",
    "print(\"Play move scores..\")\n",
    "print(play)\n",
    "\n",
    "print(\"Transitions..\")\n",
    "print(transitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the route from the run.\n",
    "\n",
    "### if at all there is going to be any visited node already then, pick the next highly possible move from all the moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 --> 5 --> 10 --> 11 --> 12 --> 13 --> 8 --> 9 --> "
     ]
    }
   ],
   "source": [
    "optimalrouteToTreasure = []\n",
    "currentstate = 0\n",
    "previousState=0\n",
    "nextHighestValue=0\n",
    "state = 0\n",
    "\n",
    "while state != 4: #(terminal state)\n",
    "    row = play.iloc[state]\n",
    "    print(state, \"-->\", end=\" \")\n",
    "    # get the index maximum value from the dataframe row\n",
    "    action = row.idxmax(axis=1)\n",
    "    #transitions of current state and the action will identify the next state\n",
    "    state = transitions[state][action]\n",
    "\n",
    "    ## trying to avoid previously visited nodes if any.\n",
    "    if (state in optimalrouteToTreasure):\n",
    "        nextHighest = 4 #(next highest in 5(0-4) values is 3, so setting this to 4 and subtracting below)\n",
    "        while(state in optimalrouteToTreasure):\n",
    "            nextHighest=nextHighest-1\n",
    "            nextHighestValue = np.sort(play.iloc[previousState])[nextHighest] # get next highest value\n",
    "            i=0\n",
    "            for value in row.tolist():    \n",
    "                if value == nextHighestValue:\n",
    "                    state = transitions[previousState][i]\n",
    "                    break\n",
    "                i=i+1\n",
    "                \n",
    "    optimalrouteToTreasure.append(state)\n",
    "    previousState = state    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal route for the Player to reach the treasure from source  is  [5, 10, 11, 12, 13, 8, 9, 4]\n"
     ]
    }
   ],
   "source": [
    "print(\"The optimal route for the Player to reach the treasure from source  is \", optimalrouteToTreasure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
